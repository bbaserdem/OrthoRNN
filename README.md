# OrthoRNN
An orthonormal RNN. To be studied.

The main idea is; the first hidden layer is a large dimensional, linear layer with recurrency specifically being an orthogonal matrix. Things to research;

	Convergence (with stable orthogonal layer)
	Orthogonal layer learning & faster teaching methods.
	Dimensionality dependence
	Decay through the orthogonal layer?
